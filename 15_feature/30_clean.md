# 数据清洗

好的数据分析师必定是一名数据清洗高手，要知道在整个数据分析过程中，不论是在时间还是功夫上，数据清洗大概都占到了 80%。

质量准则：

- 完整性：单条数据是否存在空值，统计的字段是否完善。
- 全面性：观察某一列的全部数值，比如在 Excel  表中，我们选中一列，可以看到该列的平均值、最大值、最小值。我们可以通过常识来判断该列是否有问题，比如：数据定义、单位标识、数值本身。
- 合法性：数据的类型、内容、大小的合法性。比如数据中存在非 ASCII 字符，性别存在了未知，年龄超过了 150  岁等。
- 唯一性：数据是否存在重复记录，因为数据通常来自不同渠道的汇总，重复的情况是常见的。行数据、列数据都需要是唯一的，比如一个人不能重复记录多次，且一个人的体重也不能在列指标中重复记录多次。

按照以上的原则，我们能解决数据清理中遇到的大部分问题，使得数据标准、干净、连续，为后续数据统计、数据挖掘做好准备。

## 完整性

### 缺失值

特征有缺失值是非常常见的，大部分机器学习模型在拟合前需要所有的特征都有值，不能是空或为 NULL。在数据中有些年龄、体重数值是缺失的，这往往是因为数据量较大，在过程中，有些数值没有采集到。通常我们可以采用以下三种方法：

- 删除：删除数据缺失的记录
- 代替：在 sklearn 中，可以使用 preprocessing.Imputer 来做预处理
  - 如果是连续值：
    - 选择所有有该特征值的样本，然后取平均值来填充缺失值
    - 取中位数来填充缺失值。
  - 如果是离散值：则一般会选择所有有该特征值的样本中最频繁出现的类别值，来填充缺失值。

比如我们想对 df[‘Age’]中缺失的数值用平均年龄进行填充，可以这样写：

```python
df['Age'].fillna(df['Age'].mean(), inplace=True)
```

### 空行

我们发现数据中有一个空行，除了 index 之外，全部的值都是 NaN。Pandas 的 read_csv() 并没有可选参数来忽略空行，这样，我们就需要在数据被读入之后再使用 dropna() 进行处理，删除空行。

```python
df.dropna(how='all',inplace=True) 
```

## 全面性

### 单位不统一

```python
# 获取 weight 数据列中单位为 lbs 的数据
rows_with_lbs = df['weight'].str.contains('lbs').fillna(False)
print df[rows_with_lbs]
# 将 lbs转换为 kgs, 2.2lbs=1kgs
for i,lbs_row in df[rows_with_lbs].iterrows():
# 截取从头开始到倒数第三个字符之前，即去掉lbs。
weight = int(float(lbs_row['weight'][:-3])/2.2)
df.at[i,'weight'] = '{}kgs'.format(weight) 
```

### 非 ASCII 字符

可以采用删除或者替换的方式来解决非 ASCII 问题，这里我们使用删除方法：

```python
df['first_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
df['last_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
```

## 唯一性

### 重复行

校验一下数据中是否存在重复记录。如果存在重复记录，就使用 Pandas 提供的 drop_duplicates() 来删除重复数据。

```python
# 删除重复数据行
df.drop_duplicates(['first_name','last_name'],inplace=True)
```

没有高质量的数据，就没有高质量的数据挖掘，而数据清洗是高质量数据的一道保障。当你从事这方面工作的时候，你会发现养成数据审核的习惯非常重要。而且越是优秀的数据挖掘人员，越会有“数据审核”的“职业病”。这就好比编辑非常在意文章中的错别字、语法一样。数据的规范性，就像是你的作品一样，通过清洗之后，会变得非常干净、标准。

### 重复列

### 列值为常数

### 一列有多个参数

在数据中不难发现，姓名列（Name）包含了两个参数 Firstname 和 Lastname。为了达到数据整洁目的，我们将 Name 列拆分成 Firstname 和 Lastname  两个字段。我们使用 Python 的 split 方法，str.split(expand=True)，将列表拆成新的列，再将原来的 Name  列删除。

```python
df[['first_name','last_name']] = df['name'].str.split(expand=True)
df.drop('name', axis=1, inplace=True)
```



## 合理性

### 异常数据清洗

在实际项目中拿到的数据往往有不少异常数据，有时候不筛选出这些异常数据很可能让后面的数据分析模型有很大的偏差。常用的方法有两种：

1. 聚类：比如可以用 KMeans 聚类将训练样本分成若干个簇，如果某一个簇里的样本数很少，而且簇质心和其他所有的簇都很远，那么这个簇里面的样本极有可能是异常特征样本了，可以将其从训练集过滤掉。
2. 异常点检测方法：主要是使用 iForest 或 one class SVM，使用异常点检测的机器学习算法来过滤所有的异常点。

当然，某些筛选出来的异常样本是否真的是不需要的异常特征样本，最好找懂业务的再确认一下，防止将正常的样本过滤掉了。

### 数据分布不均衡

例如，异常数据只占 1%

这个问题其实不算特征预处理的部分，不过其实它的实质还是训练集中各个类别的样本的特征分布不一致的问题。

做分类算法训练时，如果训练集里的各个类别的样本数量不是大约相同的比例，就需要处理样本不平衡问题。如果不处理，那么拟合出来的模型对于训练集中少样本的类别泛化能力会很差。例如对于一个二分类问题，如果训练集里 A 类别样本占 90%，B 类别样本占 10%。而测试集里 A 类别样本占 50%， B 类别样本占 50%，如果不考虑类别不平衡问题，训练出来的模型对于类别 B 的预测准确率会很低，甚至低于50%。

一般是两种方法：

- 权重法：是对训练集里的每个类别加一个权重 class  weight。如果该类别的样本数多，那么它的权重就低，反之则权重就高。如果更细致点，还可以对每个样本加权重 sample  weight，思路和类别权重也是一样，即样本数多的类别样本权重低，反之权重高。sklearn 中，绝大多数分类算法都有 class  weight 和 sample weight。
- 采样法：有两种思路，一种是对类别样本数多的样本做子采样,  比如训练集里 A 类别样本占 90%，B 类别样本占 10%。那么可以对 A 类的样本子采样，直到子采样得到的 A 类样本数和 B 类别现有样本一致为止，这样就只用子采样得到的 A 类样本数和 B 类现有样本一起做训练集拟合模型。第二种思路是对类别样本数少的样本做过采样，还是上面的例子，对 B 类别的样本做过采样，直到过采样得到的 B 类别样本数加上 B 类别原来样本一起和 A 类样本数一致，最后再去拟合模型。

上述两种常用的采样法很简单，但是都有个问题，就是采样后改变了训练集的分布，可能导致泛化能力差。所以有的算法就通过其他方法来避免这个问题，比如 SMOTE 算法通过人工合成的方法来生成少类别的样本。方法也很简单，对于某一个缺少样本的类别，它会随机找出几个该类别的样本，再找出最靠近这些样本的若干个该类别样本，组成一个候选合成集合，然后在这个集合中不停的选择距离较近的两个样本，在这两个样本之间，比如中点，构造一个新的该类别样本。举个例子，比如该类别的候选合成集合有两个样本 (*𝑥*1,*𝑦*),(*𝑥*2,*𝑦*)，SMOTE 采样后，可以得到一个新的训练样本 $(\frac{𝑥1+𝑥2}{2},𝑦)$，通过这种方法可以得到不改变训练集分布的新样本，让训练集中各个类别的样本数趋于平衡。可以用 imbalance-learn 这个 Python 库中的SMOTEENN 类来做 SMOTE 采样。

### 样本次序混乱

行的次序被打乱了，或则一部分样本被修改了

the dataset is shuffled



### 样本权重不均


## Lab
- [数据清洗1](30_clean.ipynb)
- [数据清洗 springleaf](31_springleaf-clean.ipynb)


