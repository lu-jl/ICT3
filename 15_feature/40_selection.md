# 特征提取

在特征选择前，需要先对字段进行筛选，然后对数据进行探索和相关性分析，接着是选择算法模型（这里暂时不需要进行模型计算），然后针对算法模型对数据的需求进行数据变换，从而完成机器学习前的准备工作。

<img src="figures/image-20210202083242084.png" alt="image-20210202083242084" style="zoom:50%;" />

## 单特征变换

需要让数据满足一定的规律，达到规范性的要求，便于进行训练，这就是特征变换的作用。

### 类别特征

类别型特征（Categorical Feature）主要是指性别（男、女）、血型（A、B、 AB、O）等只在有限选项内取值的特征。类别型特征原始输入通常是字符串形 式，除了决策树等少数模型能直接处理字符串形式的输入，对于逻辑回归、支持  向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作。

#### 数据概化

将数据由较低的概念抽象成为较高的概念，减少数据复杂度，即用更高的概念替代更低的概念。比如说上海、杭州、深圳、北京可以概化为中国。

#### 序号编码

序号编码通常用于处理类别间具有大小关系的类别数据。例如成绩，可以分为 低、中、高三档，并且存在“高>中>低”的排序关系。序号编码会按照大小关系对类别型特征赋予一个数值ID，例如高表示为 3、中表示为 2、低表示为 1，转换后依然保留了大小关系。

#### One-Hot 编码

one-hot 编码通常用于处理类别间不具有大小关系的特征。例如血型，一共有4个取值（A型血、B型血、AB型血、O型血），one-hot 编码会把血型变成一个 4 维稀疏向量，A 型血表示为（1, 0, 0, 0），B 型血表示为（0, 1, 0, 0），AB 型表示为（0, 0, 1, 0），O 型血表示为（0,  0, 0, 1）。对于类别取值较多的情况下使用独热编码。

#### 二进制编码

二进制编码主要分为两步，先用序号编码给每个类别赋予一个类别 ID，然后将类别 ID 对应的二进制编码作为结果。以 A、B、AB、O 血型为例：A 型血的 ID 为 1，二进制表示为 001；B 型血的 ID 为 2，二进制表示为 010；以此类推可以得到 AB 型血和 O 型血的二进制表示。

### 规范化

数据规范化是将不同渠道的数据，都按照同一种尺度来进行度量，这样做有两个好处，一是让数据之间具有可比较性；另一个好处就是方便后续运算，因为数据在同一个数量级上规整了，在机器学习迭代的时候，也会加快收敛效率。规范化具体是使属性数据按比例缩放，这样就将原来的数值映射到一个新的特定区域中。常用的方法有归一化、标准化等。

#### log

#### 指数

#### box-cox

#### 线性归一化

为了消除数据特征之间的量纲影响，需要对特征进行归一化处理，使得不同指标之间具有可比性。例如，分析一个人的身高和体重对健康的影响，如果使用米（m）和千克（kg）作为单位，那么身高特征会在 1.6～1.8m 的数值范围内，体重特征会在 50～100kg 的范围内，分析出来的结果显然会倾向于数值差别比较大的体重特征。想要得到更为准确的结果，就需要进行特征归一化  （Normalization）处理，使各指标处于同一数值量级，以便进行分析。

线性函数归一化（Min-Max Scaling）是一种缩放技术，对原始数据进行线性变换，使结果映射到 [0, 1] 的范围，实现对原始数据的等比缩放，用公式表示就是：$X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}}$。

```python
from sklearn import preprocessing
import numpy as np
# 初始化数据，每一行表示一个样本，每一列表示一个特征
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行 min-max 规范化
min_max_scaler = preprocessing.MinMaxScaler()
minmax_x = min_max_scaler.fit_transform(x)
print minmax_x
```

#### 零均值归一化

零均值（Z-score）归一化，也叫标准化，是一种缩放技术，它会将原始数据映射到均值为 0、标准差为 1 的正态分布上。

假设原始特征的均值为 $\mu$、标准差为 $\sigma$，那么 Z-Score 归一化的公式为：$z=\frac{x-\mu}{\sigma}$。

训练数据归一化后，容易更快地通过梯度下降找到最优解。

<img src="figures/image-20210321145854245.png" alt="image-20210321145854245" style="zoom:33%;" />

代码如下：

```python
from sklearn import preprocessing
import numpy as np
# 初始化数据
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行Z-Score规范化
scaled_x = preprocessing.scale(x)
print scaled_x
```

### 离散化

去除数据中的噪声，将连续数据离散化。

- ? Dummy Coding

## 多特征

对当前学习有用的特征被称为“相关特征”，而没什么用的特征被称为“无关特征”。

### 降维

在高维情况下出现的数据样本稀疏、距离计算困难等问题是所有机器学习方法共同面临的障碍，被称为”维数灾难”。缓解维数灾难的重要途径就是降维，即通过数学变换将原始高维属性空间转变为一个低维“子空间”。在这个字空间中样本密度大幅提高，距离计算也变得更为容易。原始高维空间中的样本点，在低维子空间中更容易进行学习。

**降维的必要性**：

1. 多重共线性和预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。
2. 高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有2%。
3. 过多的变量，对查找规律造成冗余麻烦。
4. 仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。

**降维的目的**：

1. 使得数据集更容易使用
2. 降低很多算法的计算开销
3. 去除噪音
4. 使得结果易懂

#### LDA

线性判别分析 LDA（Linear Discriminant Analysis）是一种经典的降维方法。与 PCA 不考虑样本类别输出的无监督降维技术不同，LDA 是一种监督学习的降维技术，数据集的每个样本都有标签。

LDA 分类思想主要是：在多维空间中数据处理分类问题较为复杂，LDA 将多维空间中的数据投影到一条直线上，将 d 维数据转化成 1 维数据进行处理。

1. 对训练数据：设法将多维数据投影到一条直线上，同类数据的投影点尽可能接近，异类数据点尽可能远离。
2. 对测试数据：将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。

如果用一句话概括 LDA 思想即：投影后类内方差最小，类间方差最大。

#### PCA

主成分分析 PCA（Principal Component Analysis）是常用的降维方法，其主要原理就是找出一个最主要的特征，然后进行分析，可用于提取数据的主要特征分量。

##### 原理

1. 找出第一个主成分向量，也就是数据 `方差最大` 的向量。
2. 找出第二个主成分向量，也就是数据 `方差次大` 的向量，并且该向量与第一个主成分向量 `正交(orthogonal 如果是二维空间就叫垂直)`。
3. 通过这种方式计算出所有的主成分向量。

通过数据集的协方差矩阵及其特征值分析，就可以得到这些主成分的向量。一旦得到了协方差矩阵的特征值和特征向量，就可以保留最大的 N 个特征。这些特征向量也给出了 N 个最重要特征的真实结构，就可以通过将数据乘上这 N 个特征向量从而将它转换到新的空间上。
##### 向量基础

- 内积：向量 A 与 向量 B 的内积等于 A 到 B 的投影长度乘以 B 的模。如果 $|B|=1$，则 A 与 B 的内积等于 A 向 B 所在直线投影的标量大小。
- 基：要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值就可以了。为了方便求坐标，设定这组基向量模长为 1。因为向量的内积运算，当模长为 1 时，内积可以直接表示投影。还需要这组基是线性无关的，一般用正交基，非正交的基也是可以的，不过正交基有较好的性质。
- 基变换：$p_i$ 为转换向量，$a_M$ 为现有向量坐标。两个矩阵相乘的意义是将右边矩阵中的每一列向量变换到左边矩阵中以每一行行向量为基所表示的空间中去。<img src="figures/image-20210321101317497.png" alt="image-20210321101317497" style="zoom:33%;" />

##### 最大可分性

以上讨论了选择不同的基可以对同样一组数据给出不同的表示，如果基的数量少于向量本身的维数，则可以达到降维的效果。但关键问题在于：如何选择基才是最优的。一种直观的看法是：希望投影后的投影值尽可能分散，因为如果重叠就会有样本消失，也就是“最大可分性”。

- 方差：值的分散程度可以用方差来表述，一个变量的方差可以看做是每个元素与变量均值的差的平方和的均值，即：<img src="figures/image-20210321101801456.png" alt="image-20210321101801456" style="zoom: 33%;" />。将每个变量的均值都化为 0，则可表示为：<img src="figures/image-20210321101858292.png" alt="image-20210321101858292" style="zoom: 33%;" />。于是上面的问题被表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。
- 协方差：在一维空间中可以用方差来表示数据的分散程度。而对于高维数据，可以用协方差进行约束，协方差可以表示两个变量的相关性。<img src="figures/image-20210321102217739.png" alt="image-20210321102217739" style="zoom:33%;" />。当均值为 0 时，可表示为：<img src="figures/image-20210321102301702.png" alt="image-20210321102301702" style="zoom:33%;" />。当协方差为 0 时，表示两个变量完全独立。为了让协方差为 0，选择第二个基时只能在与第一个基正交的方向上进行选择，因此最终选择的两个方向一定是正交的。

降维问题的优化目标：将一组 N 维向量降为 K 维，其目标是选择 K 个单位正交基，使得原始数据变换到这组基上后，变量方差则尽可能大（在正交的约束下，取最大的 K 个方差），而各变量两两间协方差为 0。

##### 协方差矩阵

假设我们只有 a 和 b 两个变量，那么将它们按行组成矩阵 X：<img src="figures/image-20210321104004348.png" alt="image-20210321104004348" style="zoom:33%;" />，然后：<img src="figures/image-20210321104027056.png" alt="image-20210321104027056" style="zoom:33%;" />。可以看到这个矩阵对角线上的分别是两个变量的方差，而其它元素是 a 和 b 的协方差，两者被统一到了一个矩阵里。

设我们有 m 个 n 维数据记录，将其排列成矩阵 $X_{n,m}$，设 $C=\frac{1}{m}XX^T$，则 C 是一个对称矩阵，其对角线分别对应各个变量的方差，而第 i 行 j 列和 j 行 i 列元素相同，表示 i 和 j 两个变量的协方差。

根据优化条件，需要将除对角线外的其它元素化为 0，并且在对角线上将元素按大小从上到下排列（变量方差尽可能大），这样就达到了优化目的。

##### 变换后协方差矩阵

设原始数据矩阵 X 对应的协方差矩阵为 C，而 P 是一组基按行组成的矩阵，设 Y=PX，则 Y 为 X 对 P 做基变换后的数据。设 Y 的协方差矩阵为 D，我们推导一下 D 与 C 的关系：

<img src="figures/image-20210321110728818.png" alt="image-20210321110728818" style="zoom:33%;" />

我们要找的 P 是能让原始协方差矩阵 C 对角化的 P。换句话说，优化目标变成了寻找一个矩阵 P，满足 $PCP^T$ 是一个对角矩阵，并且对角元素按从大到小依次排列，那么 P 的前 K 行就是要寻找的基，用 P 的前 K 行组成的矩阵乘以 X 就使得 X 从 N 维降到了 K 维并满足上述优化条件。

##### 求解 P 

因为协方差矩阵 C 是一个是对称矩阵，所有有：

- 实对称矩阵不同特征值对应的特征向量必然正交。
- 设特征向量 $\lambda$ 重数为 r，则必然存在 r 个线性无关的特征向量对应于 $\lambda$，因此可以将这 r 个特征向量单位正交化。

由上面两条可知，一个 n 行 n 列的实对称矩阵一定可以找到 n 个单位正交特征向量，设这 n 个特征向量为 $e_1,e_2, \dots,e_n$ ，将其按列组成矩阵： $E=(e_1,e_2,\dots,e_n)$，则对协方差矩阵 C 有：<img src="figures/image-20210321111338613.png" alt="image-20210321111338613" style="zoom:33%;" />

到这里，我们已经找到了需要的矩阵 $P=E^T$ 。P 是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是 C 的一个特征向量。如果设 P 按照  $E^TCE$ 中特征值的从大到小，将特征向量从上到下排列，则用 P 的前 K 行组成的矩阵乘以原始数据矩阵 X，就得到了需要的降维后的数据矩阵 Y。

设有 m 条 n 维数据，具体步骤为：

1. 将原始数据按列组成 n 行 m 列矩阵 X；
2. 将 X 的每一行进行零均值化，即减去这一行的均值；
3. 求出协方差矩阵 $C=\frac{1}{m}XX^T$；
4. 求出协方差矩阵的特征值及对应的特征向量；
5. 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P；
6. $Y=PX$  即为降维到 k 维后的数据。

#### 因子分析

因子分析（Factor Analysis）将多个变量转换为少数几个综合指标。它反映一种降维的思想，通过降维将相关性高的变量聚在一起，从而减少需要分析的变量的数量，减少问题分析的复杂性

例如:  考察一个人的整体情况，就直接组合 3 样成绩看平均成绩就行（ 数学、语文、英语成绩）。

在因子分析中，我们假设观察数据的成分中有一些观察不到的隐变量（latent variable），假设观察数据是这些隐变量和某些噪音的线性组合，那么隐变量的数据可能比观察数据的数目少，也就说通过找到隐变量就可以实现数据的降维。

### 特征选择

从给定的特征集合中选择出相关特征自己的过程被称为“特征选择”，它们往往是根据所需的训练模型选择所需的特征。

- 若能选择出重要的特征使得后续学习过程仅需在一部分特征上构建模型，则维数灾难问题会大为减轻
- 除去不相关特征往往会降低学习任务的难度

特征选择的过程必须确保不丢失重要的特征，否则后续学习过程会因为重要信息的缺失而无法获得好的性能。

#### 选择方法

特征选择的过程大致分为2步：

- 产生一个候选子集：往往可以采用“前向”（逐步增加）及“后向”（逐步减少）搜索
- 评价该候选子集的好坏：通过信息增益来作为评价准则
- 基于评价结果产生下一个候选子集

例如决策树就是很好的特征选择方法。

#### 过滤式

过滤式方法先对数据集进行特征选择，然后再进行训练学习器。因为特征选择与后续的学习器无关，这相当于先用特征选择过程对初始数据集进行过滤，再用过滤后的特征来训练模型。

#### 包裹式

包裹式特征选择直接把最终将要使用的学习器的性能作为特征子集的评价标准。换言之，包裹式特征选择的目的就是为了给定学习器选择最有利于其性能的、“量身定做”的特征子集。

由于在特征选择过程中需要多次训练学习器，因此包裹式特征选择的计算开销通常比过滤式大很多。

#### 嵌入式

嵌入式特征选择将特征选择的过程域学习器训练过程融为一体，两者在同一个优化过程中完成，即在学习训练过程中自动地进行了特征选择。

稀疏解即意味着初始的 d 个特征中仅有非零的特征才回出现在最终模型中，为 0 的特征不需要使用。如 L1 范数比 L2 范数更易于得到稀疏解，求解 L1 的结果得到了仅采用一部分初始特征的模型。换言之，基于 L1 正则化的学习方法就是一种嵌入式特征选择，其特征选择过程与学习器训练过程融为一体，同时完成。

- 树模型筛选：RF/XGBoost特征重要性

#### 稀疏表示与字典学习

特征选择所考虑的问题是特征具有“稀疏性”，即许多特征与学习任务无关，通过特征选择去除这些特征，则学习器训练过程仅需在较小的特征集中进行，学习任务的难度可能有所降低，设计的计算和存储开心会减少，学得模型的可解释性也会提高。

所谓字典学习（稀疏编码），就是为普通稠密表达的样本（即每行很多不为 0 项）找到合适的字典，将样本转化为合适的稀疏表示形式，从而使得学习任务得以简化。

## 衍生特征

衍生特征就是构造出新的特征并添加到特征集中。这里会用到特征工程的知识，因为通过特征与特征的连接构造新的特征。比如说，数据表中统计每个人的英语、语文和数学成绩，你可以构造一个“总和”这个属性，来作为新属性。这样“总和”这个属性就可以用到后续的数据挖掘计算中。

### 非线性可分转线性可分



### 二次元假设

- 从一元X空间变换到二元Z空间
- 在Z空间使用PLA：对应X空间的二次曲线



## Ref

1. [ML特征工程和优化方法](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/8.%20ML%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E5%92%8C%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95)
2. 