# 特征选择

在特征选择前，需要先对字段进行筛选，然后对数据进行探索和相关性分析，接着是选择算法模型（这里暂时不需要进行模型计算），然后针对算法模型对数据的需求进行数据变换，从而完成机器学习前的准备工作。

<img src="figures/image-20210202083242084.png" alt="image-20210202083242084" style="zoom:50%;" />

## 单特征变换

需要让数据满足一定的规律，达到规范性的要求，便于进行训练，这就是特征变换的作用。

### 编码

#### One-Hot 编码

将分类数据转换为列，并将类别作为该列的值。

#### 标签编码

通过为每个类别分配一个唯一的整数值，将分类数据转换为数字，称为标签编码。

### 规范化

数据规范化是将不同渠道的数据，都按照同一种尺度来进行度量，这样做有两个好处，一是让数据之间具有可比较性；另一个好处就是方便后续运算，因为数据在同一个数量级上规整了，在机器学习迭代的时候，也会加快收敛效率。规范化具体是使属性数据按比例缩放，这样就将原来的数值映射到一个新的特定区域中。常用的方法有归一化、标准化等。

#### log

#### 指数

#### 归一化

归一化（也称为最小-最大归一化）是一种缩放技术，它将特征重新标定，使数据落在 [0,1] 的范围内。数据归一化会让数据在一个 [0,1] 或者 [-1,1] 的区间范围内。

Min-max 规范化方法是将原始数据变换到 [0,1] 的空间中，用公式表示就是：$新数值=\frac{原数值 - 极小值}{极大值 - 极小值}$。

```python
from sklearn import preprocessing
import numpy as np
# 初始化数据，每一行表示一个样本，每一列表示一个特征
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行 min-max 规范化
min_max_scaler = preprocessing.MinMaxScaler()
minmax_x = min_max_scaler.fit_transform(x)
print minmax_x
```

#### 标准化

标准化（Z-score归一化）是一种缩放技术，它将特征重新调整，使其具有标准正态分布，即均值为0、标准差为=1。

Z-Score 规范化的公式为：$z=\frac{x-\mu}{\sigma}$。其中，*μ* 为平均值（average），*σ*为与平均值的标准差。

```python
from sklearn import preprocessing
import numpy as np
# 初始化数据
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行Z-Score规范化
scaled_x = preprocessing.scale(x)
print scaled_x
```

#### box-cox

### 数据平滑/离散化

去除数据中的噪声，将连续数据离散化。

- ? Dummy Coding

### 数据概化

将数据由较低的概念抽象成为较高的概念，减少数据复杂度，即用更高的概念替代更低的概念。比如说上海、杭州、深圳、北京可以概化为中国。

## 多个特征

- 信息量太少
  - 缺失百分比太高，方差变化太小
  - 解决：去掉

#### 降维

多个特征聚类做新特征

X高度相关，X之间相关系数，共线性

- PCA
- LDA
- 聚类

#### 特征选择

往往是根据所需的训练模型选择所需的特征

X与Y的相关性太低

- filter
  - 相关系数
  - 卡方验证
- wrapper
- embedded
- 树模型筛选：RF/XGBoost特征重要性

## 衍生特征

衍生特征就是构造出新的特征并添加到特征集中。这里会用到特征工程的知识，因为通过特征与特征的连接构造新的特征。比如说，数据表中统计每个人的英语、语文和数学成绩，你可以构造一个“总和”这个属性，来作为新属性。这样“总和”这个属性就可以用到后续的数据挖掘计算中。

### 非线性可分转线性可分



### 二次元假设

- 从一元X空间变换到二元Z空间
- 在Z空间使用PLA：对应X空间的二次曲线

