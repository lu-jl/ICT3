# SVM

## 简介

支持向量机-SVM（Support Vector Machine）用于超平面是分割输入变量空间的线。在SVM中，选择超平面以最佳地将输入变量空间中的点与它们的类（0级或1级）分开，所以主要是针对二分类。在二维中，您可以将其视为一条线，并假设我们的所有输入点都可以被这条线完全分开。SVM学习算法找到导致超平面最好地分离类的系数。

超平面与最近数据点之间的距离称为边距。可以将两个类分开的最佳或最佳超平面是具有最大边距的线，只有这些点与定义超平面和分类器的构造有关，这些点称为支持向量。它们支持或定义超平面。对于 SVM 来说，它是最大化两个类别边距的那种方式，换句话说：超平面（在本例中是一条线）对每个类别最近的元素距离最远。





<img src="../figures/image-20200321122333643.png" alt="image-20200321122333643" style="zoom:33%;" />

### 分类间隔

在保证决策面不变，且分类不产生错误的情况下，我们可以移动决策面 C，直到产生两个极限的位置：如图中的决策面 A 和决策面 B。极限的位置是指，如果越过了这个位置，就会产生分类错误。这样的话，两个极限位置 A 和 B 之间的分界线 C 就是最优决策面。极限位置到最优决策面 C 之间的距离，就是“分类间隔”，英文叫做 margin。如果我们转动这个最优决策面，你会发现可能存在多个最优决策面，它们都能把数据集正确分开，这些最优决策面的分类间隔可能是不同的，而那个拥有“最大间隔”（max margin）的决策面就是 SVM 要找的最优解。

SVM 就是帮我们找到一个超平面，这个超平面能将不同的样本划分开，同时使得样本集中的点到这个分类超平面的最小距离（即分类间隔）最大化。在这个过程中，支持向量就是离分类超平面最近的样本点，实际上如果确定了支持向量也就确定了这个超平面。所以支持向量决定了分类间隔到底是多少，而在最大间隔以外的样本点，其实对分类都没有意义。

<img src="figures/image-20210208150041196.png" alt="image-20210208150041196" style="zoom:50%;" />

## 核函数

它可以将样本从原始空间映射到一个更高维的特质空间中，使得样本在新的空间中线性可分。这样就可以使用原来的推导来进行计算，只是所有的推导是在新的空间，而不是在原来的空间中进行。

所以在非线性 SVM 中，核函数的选择就是影响  SVM 最大的变量。最常用的核函数有线性核、多项式核、高斯核、拉普拉斯核、sigmoid  核，或者是这些核函数的组合。这些函数的区别在于映射方式的不同。通过这些核函数，我们就可以把样本空间投射到新的高维空间中。

## 多分类器拓展

### 一对多法

假设我们要把物体分成  A、B、C、D 四种分类，那么我们可以先把其中的一类作为分类 1，其他类统一归为分类 2。这样我们可以构造 4 种 SVM，分别为以下的情况：

- 样本 A 作为正集，B，C，D 作为负集；
- 样本 B 作为正集，A，C，D 作为负集；
- 样本 C  作为正集，A，B，D 作为负集；
- 样本 D 作为正集，A，B，C 作为负集。

这种方法，针对 K 个分类，需要训练 K  个分类器，分类速度较快，但训练速度较慢，因为每个分类器都需要对全部样本进行训练，而且负样本数量远大于正样本数量，会造成样本不对称的情况，而且当增加新的分类，比如第 K+1 类时，需要重新对分类器进行构造。

### 一对一法

一对一法的初衷是想在训练的时候更加灵活。可以在任意两类样本之间构造一个 SVM，这样针对 K 类的样本，就会有 C(k,2) 类分类器。比如我们想要划分 A、B、C 三个类，可以构造 3 个分类器：

- 分类器  1：A、B；
- 分类器 2：A、C；
- 分类器 3：B、C。

当对一个未知样本进行分类时，每一个分类器都会有一个分类结果，即为 1 票，最终得票最多的类别就是整个未知样本的类别。这样做的好处是，如果新增一类，不需要重新训练所有的  SVM，只需要训练和新增这一类样本的分类器。而且这种方式在训练单个 SVM 模型的时候，训练速度快。但这种方法的不足在于，分类器的个数与 K  的平方成正比，所以当 K 较大时，训练和测试的时间会比较慢。