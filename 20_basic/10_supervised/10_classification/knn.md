# KNN

KNN-K近邻（K Nearest Neighbour）算法是基本的监督学习分类算法。

### 原理

对于一个需要分类的数据，将其和一组已经分类标注好的样本集合进行比较，得到距离最近的 K 个样本，K 个样本最多归属的类别就是这个需要分类数据的类别。

<img src="figures/image-20201127132630576.png" alt="image-20201127132630576" style="zoom: 33%;" />



KNN 算法的关键是要比较需要分类的数据与样本数据之间的距离，通常的做法是：提取数据的特征值，根据特征值组成一个 n 维实数向量空间（特征空间），然后计算向量之间的空间距离。空间之间的距离计算方法有很多种，常用的有欧氏距离、余弦距离等。

对于数据 x_i 和 x_j，若其特征空间为 n 维实数向量空间 R^n，即 x_i=(x_i1,x_i2,…,x_in)，xj=(x_j1,x_j2,…,x_jn)，则其欧氏距离计算公式为：<img src="figures/image-20201127133120342.png" alt="image-20201127133120342" style="zoom:25%;" />

在文本数据以及用户评价数据的机器学习中，更常用的距离计算方法是余弦相似度：<img src="figures/image-20201127133218934.png" alt="image-20201127133218934" style="zoom:25%;" />。余弦相似度其实是计算向量的夹角，余弦相似度更关注数据的相似性。余弦相似度的值越接近 1 表示其越相似，越接近 0 表示其差异越大









