# KNN

KNN-K 近邻（K Nearest Neighbour）算法是基本的监督学习分类算法，用于多分类。



## 算法

KNN 算法主要要考虑三个重要的要素：k 值的选取、距离度量的方式、分类决策规则。

### 分类决策规则

一般都是使用前面提到的多数表决法。

### k 值的选择

没有一个固定的经验，一般根据样本的分布，选择一个较小的值，可以通过交叉验证选择一个合适的 k 值。

- 选择较小的 k 值，就相当于用较小的领域中的训练实例进行预测，训练误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用。与此同时带来的问题是泛化误差会增大，K 值的减小就意味着整体模型变得复杂，容易发生过拟合；
- 选择较大的 k 值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少泛化误差，但缺点是训练误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且 K 值的增大就意味着整体的模型变得简单。一个极端是 k 等于样本数 m，则完全没有分类，此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单。

### 距离度量

KNN 算法的关键是要比较需要分类的数据与样本数据之间的距离，通常的做法是：提取数据的特征值，根据特征值组成一个 n 维实数向量空间（特征空间），然后计算向量之间的空间距离。空间之间的距离计算方法有很多种，常用的有欧氏距离、余弦距离等。

有很多的距离度量方式，但是最常用的是欧式距离。大多数情况下，欧式距离可以满足需求，不需要再去操心距离的度量。当然也可以用其他的距离度量方式，比如曼哈顿距离。可以看出，欧式距离是闵可夫斯基距离距离在 p=2 时的特例，而曼哈顿距离是 p=1 时的特例。

## 算法实现

### Brute-force

对于一个需要分类的数据，将其和一组已经分类标注好的样本集合进行比较，得到距离最近的 K 个样本，K 个样本最多归属的类别就是这个需要分类数据的类别。整个计算过程分为三步：

- 计算待分类物体与其他物体之间的距离；
- 统计距离最近的 K 个邻居；
- 对于 K 个最近的邻居，它们属于哪个分类最多，待分类物体就属于哪一类。

### KD 树

KD 树算法没有一开始就尝试对测试样本分类，而是先对训练集建模，建立的模型就是 KD 树，建好了模型再对测试集做预测。所谓的 KD 树就是 K 个特征维度的树，注意这里的 K 和 KNN 中的 K 的意思不同。KNN 中的 K 代表最近的 K 个样本，KD 树中的 K 代表样本特征的维数。为了防止混淆，后面称特征维数为 n。

KD 树算法包括三步，第一步是建树，第二部是搜索最近邻，最后一步是预测。

### 球树

KD 树算法虽然提高了 KNN 搜索的效率，但是在某些时候效率并不高，比如当处理不均匀分布的数据集时，不管是近似方形，还是矩形，甚至正方形，都不是最好的使用形状，因为他们都有角。为了优化超矩形体导致的搜索效率的问题，牛人们引入了球树，这种结构可以优化上面的这种问题。





## 回归

KNN 不仅可以做分类，还可以做回归。要预测某个属性值，会先计算待测点到已知点的距离，选择距离最近的 K 个点，那么它的特征值就是这 k 个点的该属性值的平均值









