# 分类算法

## 简介

### 生成 vs. 判别

- 判别方法：由数据直接学习决策函数或条件概率分布。其基本思想是在有限样本条件下建立判别函数，不考虑样本的产生模型。不能反映训练数据本身的特性，但它可以寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。
- 生成方法：从数据学习“联合概率密度分布”，然后求出条件概率分布作为预测模型，这类方法需要样本非常多时才能很好的描述数据的真正分布。之所以成为生成方法，是因为模型表示了给定输入 X 产生输出 Y 的生成关系。它从统计学的角度表示数据的分布情况，能够反映同类数据本身的相似度，但不关心划分各类的边界在哪里。

### 常用算法

分类（Classification）与回归最大的区别在于，不同的分类之前没有任何关联。

- 逻辑回归：常用
- 朴素贝叶斯（Naive Bayes）：用于 NLP
- 支持向量机 SVM（Support Vector Classifier）：中小型数据集表现好
- 决策树（Decision Tree Classifier）：常用
- 随机森林（Random Forest）：
- K-近邻算法 KNN（K-Nearest Neighbors）：较少用

## 算法

<img src="figures/image-20201115113106223.png" alt="image-20201115113106223" style="zoom: 25%;" />

## 多分类器拓展

### 一对多法

假设我们要把物体分成  A、B、C、D 四种分类，那么我们可以先把其中的一类作为分类 1，其他类统一归为分类 2。这样我们可以构造 4 种 SVM，分别为以下的情况：

- 样本 A 作为正集，B，C，D 作为负集；
- 样本 B 作为正集，A，C，D 作为负集；
- 样本 C  作为正集，A，B，D 作为负集；
- 样本 D 作为正集，A，B，C 作为负集。

这种方法，针对 K 个分类，需要训练 K  个分类器，分类速度较快，但训练速度较慢，因为每个分类器都需要对全部样本进行训练，而且负样本数量远大于正样本数量，会造成样本不对称的情况，而且当增加新的分类，比如第 K+1 类时，需要重新对分类器进行构造。

### 一对一法

一对一法的初衷是想在训练的时候更加灵活。可以在任意两类样本之间构造一个 SVM，这样针对 K 类的样本，就会有 C(k,2) 类分类器。比如我们想要划分 A、B、C 三个类，可以构造 3 个分类器：

- 分类器  1：A、B；
- 分类器 2：A、C；
- 分类器 3：B、C。

当对一个未知样本进行分类时，每一个分类器都会有一个分类结果，即为 1 票，最终得票最多的类别就是整个未知样本的类别。这样做的好处是，如果新增一类，不需要重新训练所有的  SVM，只需要训练和新增这一类样本的分类器。而且这种方式在训练单个 SVM 模型的时候，训练速度快。但这种方法的不足在于，分类器的个数与 K  的平方成正比，所以当 K 较大时，训练和测试的时间会比较慢。

## 