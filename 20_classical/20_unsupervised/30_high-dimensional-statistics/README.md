# 降维

在机器学习和统计学领域，降维是指在某些限定条件下，降低随机变量个数，得到一组“不相关”主变量的过程。 降维可进一步细分为变量选择和特征提取两大方法。 

## 变量选择

变量选择假定数据中包含大量冗余或无关变量（或称特征、属性、指标等），旨在从原有变量中找出主要变量。现代统计学中对变量选择的研究文献，大多集中于高维回归分析，其中最具代表性的方法包括：

- Lasso (Robert Tibshirani提出)
- Elastic net (邹晖和Trevor Hastie提出)
- SCAD (范剑青和李润泽提出)
- SURE screening (范剑青和吕金翅提出)
- PLUS (张存惠提出)

## 特征提取

特征提取可以看作变量选择方法的一般化：变量选择假设在原始数据中，变量数目浩繁，但只有少数几个真正起作用；而特征提取则认为在所有变量可能的函数(比如这些变量各种可能的线性组合)中，只有少数几个真正起作用。有代表性的方法包括：

- 主成分分析(PCA)
- 因子分析
- 核方法(教科书中称为“Kernel method”或“Kernel trick”，常与其他方法如PCA组合使用)
- 基于距离的方法，例如：
  - 多维尺度分析
  - 非负矩阵分解
  - 随机投影法(理论依据是约翰逊-林登斯特劳斯定理






