# CNN

## 简介

卷积神经网络 CNN（Convolutional Neural Network）属于前馈神经网络，其特点是每层的神经元节点只响应前一层局部区域范围内的神经元（FNN 全连接网络中每个神经元节点则是响应前一层的全部节点）。在数学中，卷积是一个函数越过另一个函数时两个函数重叠多少的积分度量。

一个深度卷积神经网络模型，一般由若干卷积层叠加若干全连接层组成，中间包含各种的非线性操作、池化操作。

- 卷积运算主要用于处理网格结构的数据，那就是 CNN 是利用卷积层（滤波器 Filter）自动抽取特征，将相邻像素之间的轮廓过滤出来。实际上每个卷积核都是一种滤波器，它们把图像中符合条件的部分筛选出来，也就相当于做了某种特征提取。在 CNN 的卷积层中可以有多个卷积核，以 LeNet 为例，它的第一层卷积核有 6 个，因此可以帮我们提取出图像的 6 个特征，从而得到 6 个特征图（feature maps）。
- 利用池化操作，将复杂问题简化，把大量参数降维成少量参数再做处理。重要的是在大部分场景下，降维并不会影响结果。比如1000像素的图片缩小成200像素，并不影响肉眼认出来图片中是一只猫还是一只狗，机器也是如此。

### 图形处理 3 大特征

1. pattern 要比原图小
2. pattern 可以出现在原图的各个位置
3. 压缩原图（subsampling）不会改变对其的认知

其中 1、2 通过卷积（convolution）解决，而 3 通过池化（pooling）解决。

## 结构

CNN 符合多个“卷积层+池化层”对 input 进行加工，然后再连接到一个连接层实现与输出目标之间的映射。

<img src="figures/image-20201128185217198.png" alt="image-20201128185217198" style="zoom: 33%;" />

### 卷积层

卷积层的目的是**自动提取特征**，它的运算过程用一个 filter（卷积核）扫完整张图片，这个过程可以理解为使用一个 filter 来过滤图像的各个小区域，从而得到这些小区域的特征值。

每次卷积操作，可以对相同的原图使用多个不同的 filter。每个 filter 卷积计算的结果就是卷积层中的一个 feature map，因此每个卷积层可以包含多个 feature map，其数量等于 filter 的数量，也被称为“频道数”。

在具体应用中，往往有多个 filter，每个 filter 代表了一种图像模式，如果某个图像块与此 filter 卷积出的值大，则认为此图像块十分接近于此 filter。如果设计了 6 个 filter，可以理解：认为这个图像上有 6 种底层纹理模式，也就是用 6 种基础模式就能描绘出一副图像。

<img src="figures/2019-06-19-juanji.gif" alt="卷积层运算过程" style="zoom:50%;" />

卷积的 filter 可以看做是一个window，例如有一个 6X6 的网络以及一个 3X3 的 filter，其中 filter 的每个格子上有权值。拿着 filter 在网络上去移动，直到所有的小格子都被覆盖到，每次移动，都将 filter “观察”到的内容，与之权值相乘作为结果输出。最后可以得到一个 4X4 的网格矩阵。

可以认为卷积是全连接的一种特殊情况，1/ 只让一部分 weight 有权值，2/ 让多个 weight 共享相同的权重值。

- 填充：卷积后的矩阵大小与一开始的不一致，那么需要对边缘进行填充（Padding）以保证尺寸一致。
- 步长：步长（Stride）就是 filter 移动的步伐大小，上面的例子为1，其实可以指定，有点像是学习率。
- 深度：深度（Depth）指的是图片的深度，一张 6X6X3 大小的图片经过 3X3X3 的 Filter 过滤后会得到一个 4X4X1 大小的图片，因此深度为 1。也可以通过增加 filter 的个数来增加深度。

### ？？？激活函数

激活函数是卷积操作的最后一步，其作用是在做完卷积操作之后，通常还需要使用激活函数对图像进一步处理。在逻辑回归中使用的 Sigmoid 函数在深度学习中有广泛的应用，除了 Sigmoid 外，tanh、ReLU  都是常用的激活函数。这些激活函数通常都是非线性的函数，使用它们的目的是把线性数值映射到非线性空间中。卷积操作实际上是两个矩阵之间的乘法，得到的结果也是线性的。只有经过非线性的激活函数运算之后，才能映射到非线性空间中，这样也可以让神经网络的表达能力更强大。

### 池化层

池化层通常在两个卷积层之间，它的作用相当于对神经元的数据做降维处理，这样就能降低整体计算量。其目的是**数据降维**，避免过拟合，它可以大大降低数据的维度。即使做完了卷积，图像仍然很大（因为 filter 比较小），所以为了降低数据维度，就进行下采样。池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。

<img src="figures/2019-06-19-chihua.gif" alt="池化层过程" style="zoom: 33%;" />

最常用的池化有 Maxpooling，其原理如下图：

<img src="figures/812632-20191105213631909-209754609.jpg" alt="file" style="zoom: 67%;" />

### 连接层

在神经网络中，可以叠加多个卷积层和池化层来提取更抽象的特征。经过几次卷积和池化之后，通常会有一个或多个全连接层。全连接层将前面一层的输出结果与当前层的每个神经元都进行了连接。这样就可以把前面计算出来的所有特征，通过全连接层将输出值输送给分类器，比如 Softmax 分类器。

经过卷积层和池化层处理过的数据输入到连接层，得到最终想要的结果。经过卷积层和池化层降维过的数据，连接层才能”跑得动”，不然数据量太大，计算成本高，效率低下。

<img src="figures/2019-06-19-quanlianjie.png" alt="全连接层" style="zoom:50%;" />

### 分类器

在深度学习中，Softmax 是个很有用的分类器，通过它可以把输入值映射到 0-1 之间，而且所有输出结果相加等于  1。其实你可以换种方式理解这个概念，假设我们想要识别一个数字，从 0 到 9 都有可能。那么通过 Softmax 层，对应输出 10  种分类结果，每个结果都有一个概率值，这些概率相加为 1，我们就可以知道这个数字是 0 的概率是多少，是 1 的概率是多少……是 9  的概率又是多少，从而也就帮我们完成了数字识别的任务。

### 总结

CNN  网络结构中每一层的作用：它通过卷积层提取特征，通过激活函数让结果映射到非线性空间，增强了结果的表达能力，再通过池化层压缩特征图，降低了网络复杂度，最后通过全连接层归一化，然后连接 Softmax 分类器进行计算每个类别的概率。通常我们可以使用多个卷积层和池化层，最后再连接一个或者多个全连接层，这样也就产生了不同的网络结构，比如 LeNet 和 AlexNet。