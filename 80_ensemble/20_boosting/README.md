# Boosting

## 简介

Boosting 先从初始训练集训练出一个个体学习器，在基于个体学习器的表现对训练样本分布进行调整，使得先前的个体学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个个体学习器。如此重复进行，直至个体学习器的数目达到事先指定的值，最终将得到的所有个体学习器加权结合。

Boosting is a form of weighted averaging of models where each model is built sequentially in a way that it takes into account previous model performance. 

### 原理

Boosting 要求个体学习器能对特定的数据分布进行学习，这可以通过“重赋权法”实施，即在训练过程的每一轮中，根据样本分布为每个训练样本重新赋予一个权重。对于无法接受带权重的个体学习器，则可通过“重采样法”来处理。即在每一轮学习中，根据样本分布对训练集重新进行采样，再用采样而得的样本集对个体学习器进行训练。

### 分类

- weight based：根据 label 的值调整样本的出现频率，然后得出一个新的模型，以此类推获取新模型。最后的模型是之前所有的模型按照 learning rate（权重衰减系数）的比例叠加，例如 AdaBoost。
- residual based：基于上一轮训练的残差来计算下一个模型。

### 偏差-方差

从偏差-方差分解的角度看， Boosting 主要关注降低偏差，因此 Boosting 能基于泛化性能相当弱的学习器构建出很强的集成。

### Boosting vs. Bagging

就是集思广益，博取众长，当我们做决定的时候，先听取多个专家的意见，再做决定。集成算法通常有两种方式，分别是投票选举（bagging）和再学习（boosting）。Boosting 和 Bagging 最本质的差别在于他对基础模型不是一致对待的，而是经过不停的考验和筛选来产生“精英”，然后给精英更多的投票权，表现不好的基础模型则给较少的投票权，然后综合所有人的投票得到最终结果。

- Bagging：如果个体学习器间不存在强依赖关系，可同时生成的并行化方法。Bagging 的场景类似把专家召集到一个会议桌前，当做一个决定的时候，让 K 个专家（K 个个体分类器）分别进行分类，然后选择出现次数最多的那个类作为最终的分类结果。
- Boosting：个体分类器间存在强依赖关系、必须串行生成的序列化方法。Boosting 相当于把 K 个专家（K 个分类器）进行加权融合，形成一个新的超级专家（强分类器），让这个超级专家做判断。

大部分情况下，经过 Boosting 得到的结果偏差（bias）更小。Boosting  的含义是提升，它的作用是每一次训练的时候都对上一次的训练进行改进提升，在训练的过程中这 K 个“专家”之间是有依赖性的，当引入第 K 个“专家”（第 K 个分类器）的时候，实际上是对前 K-1 个专家的优化。而 Bagging 在做投票选举的时候可以并行计算，也就是 K 个“专家”在做判断的时候是相互独立的，不存在依赖性。

- 通过加法模型将基础模型进行线性的组合。
- 每一轮训练都提升那些错误率小的基础模型权重，同时减小错误率高的模型权重。
- 在每一轮改变训练数据的权值或概率分布，通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。

<img src="../figures/image-20200321125658497.png" alt="image-20200321125658497" style="zoom:33%;" />

## 算法

- [Adaboost](40_adaboosting.md)
- gbdt
- xgboost 