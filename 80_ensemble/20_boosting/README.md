# Boosting

## 简介

Boosting 先从初始训练集训练出一个个体学习器，在基于个体学习器的表现对训练样本分布进行调整，使得先前的个体学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个个体学习器。如此重复进行，直至个体学习器的数目达到事先指定的值，最终将得到的所有个体学习器加权结合。

### 原理

Boosting 要求个体学习器能对特定的数据分布进行学习，这可以通过“重赋权法”实施，即在训练过程的每一轮中，根据样本分布为每个训练样本重新赋予一个权重。对于无法接受带权重的个体学习器，则可通过“重采样法”来处理。即在每一轮学习中，根据样本分布对训练集重新进行采样，再用采样而得的样本集对个体学习器进行训练。

### 偏差-方差

从偏差-方差分解的角度看， Boosting 主要关注降低偏差，因此 Boosting 能基于泛化性能相当弱的学习器构建出很强的集成。

### Boosting vs. Bagging

Boosting 和 Bagging 最本质的差别在于他对基础模型不是一致对待的，而是经过不停的考验和筛选来产生“精英”，然后给精英更多的投票权，表现不好的基础模型则给较少的投票权，然后综合所有人的投票得到最终结果。大部分情况下，经过 Boosting 得到的结果偏差（bias）更小。Boosting  的含义是提升，它的作用是每一次训练的时候都对上一次的训练进行改进提升，在训练的过程中这 K 个“专家”之间是有依赖性的，当引入第 K 个“专家”（第 K 个分类器）的时候，实际上是对前 K-1 个专家的优化。而 Bagging 在做投票选举的时候可以并行计算，也就是 K 个“专家”在做判断的时候是相互独立的，不存在依赖性。

- 通过加法模型将基础模型进行线性的组合。
- 每一轮训练都提升那些错误率小的基础模型权重，同时减小错误率高的模型权重。
- 在每一轮改变训练数据的权值或概率分布，通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。

<img src="../figures/image-20200321125658497.png" alt="image-20200321125658497" style="zoom:33%;" />

## 算法

- [Adaboost](40_adaboosting.md)
- gbdt
- xgboost 