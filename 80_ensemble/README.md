# 集成学习

## 简介

集成学习通过构建并结合多个学习器来完成学习任务，其结构为先产生一个个体学习器，再用某种策略将它们结合起来。集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能，这对“弱学习器”尤为明显。

### 同质 vs. 异质

- 同质：集成中只包含同种类型的个体学习器
- 异质：集成中同时包含不同类型的个体学习器，例如同时包含决策树和神经网络

### 准确性 vs. 多样性

要获得好的集成，个体学习器应“好”（准确性）而“不同”（多样性），即个体学习器要有一定的“准确性”，而且要有“多样性”，使学习器间具有差异。

假设个体分类器的错误率相互独立，则由 Hoeffding 不等式可知，集成的错误率随着集成中个体分类器数目的增大，集成的错误率将指数级下降，最终趋向于 0。

但个体分类器的“准确性”和“多样性”本身就存在冲突。一般，准确性很高之后，要增加多样性就需牺牲准确性。如何产生“好而不同”的个体学习器是集成学习研究的核心。

### Bagging vs. Boosting

就是集思广益，博取众长，当我们做决定的时候，先听取多个专家的意见，再做决定。集成算法通常有两种方式，分别是投票选举（bagging）和再学习（boosting）。

- Bagging：如果个体学习器间不存在强依赖关系，可同时生成的并行化方法。Bagging 的场景类似把专家召集到一个会议桌前，当做一个决定的时候，让 K 个专家（K 个个体分类器）分别进行分类，然后选择出现次数最多的那个类作为最终的分类结果。
- Boosting：个体分类器间存在强依赖关系、必须串行生成的序列化方法。Boosting 相当于把 K 个专家（K 个分类器）进行加权融合，形成一个新的超级专家（强分类器），让这个超级专家做判断。

### 优缺点

#### 优点

- 从通的角度来说，学习任务的假设空间很大，可能有多个假设在训练集上达到同等性能，此时若使用单个学习器可能因误选而导致泛化性能不佳，结合多个学习器则会减少这类风险。
- 学习器往往会陷入局部极小，从而造成对应的泛化性能糟糕，而通过多个学习器结合之后，可降低局部极小点的风险。
- 有些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单个学习器则无效，而通过多个学习器的集成则有助于扩大假设空间，可能学得更好的结果。

## 结合策略

### 平均法

- 简单平均法
- 加权平均法：加权学习法可认为是集成学习的基本出发点。对应给定的个体学习器，不同的集成学习可以视为通过不同的方式来确定加权平均法中的权重。而权重一般是从训练集中学习而得。

### 投票法

主要用于分类问题

- 绝对多数投票法：某标签结果超过半数，否则拒绝预测
- 相对多数投票法：预测为得票最多的标记
- 加权投票法：

### 学习法

“学习法”结合策略通过另一种学习器来进行结合，把用于结合的学习器称为元学习器（meta-learner）。

Stacking 是学习法的典型代表，Stacking 先从初始训练集中训练出个体学习器，然后再生成一个新的训练集用于训练元学习器。在这个新的数据集中，初级学习器的输出被当做输入样例特征。

## 多样性增强

- 数据样本扰动
- 输入属性扰动
- 输出表示扰动
- 算法参数扰动：随着设置算法不同参数，如隐层数、神经元数、初始连接权值等

